#!/bin/bash

# Script de execuÃ§Ã£o completa de testes para SIA2
# Executa todos os tipos de teste e gera relatÃ³rios detalhados

# Removido set -e para permitir que testes falhos nÃ£o parem o script

# Cores para output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# FunÃ§Ã£o para print colorido
print_status() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# FunÃ§Ã£o para verificar se comando existe
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# Banner
echo "=================================================="
echo "ðŸš€ SIA2 - Sistema de AutomaÃ§Ã£o de Estufa"
echo "ðŸ§ª Pipeline Completo de Testes"
echo "=================================================="
echo

# Verificar dependÃªncias
print_status "Verificando dependÃªncias..."

DEPS_OK=true

if ! command_exists python3; then
    print_error "Python 3 nÃ£o encontrado!"
    DEPS_OK=false
fi

if ! command_exists pytest; then
    print_error "Pytest nÃ£o encontrado! Execute: pip install -r requirements-test.txt"
    DEPS_OK=false
fi

if [ "$DEPS_OK" = false ]; then
    print_error "DependÃªncias faltando. Abortando..."
    exit 1
fi

print_success "DependÃªncias verificadas"

# Criar diretÃ³rio de relatÃ³rios
REPORTS_DIR="reports"
mkdir -p $REPORTS_DIR
print_status "DiretÃ³rio de relatÃ³rios criado: $REPORTS_DIR"

# Limpar arquivos antigos
print_status "Limpando arquivos de teste antigos..."
rm -rf htmlcov .coverage .pytest_cache
rm -f $REPORTS_DIR/*.xml $REPORTS_DIR/*.json $REPORTS_DIR/*.html

# Timestamp para relatÃ³rios
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
echo "ðŸ“… Timestamp: $TIMESTAMP" >$REPORTS_DIR/test_run_$TIMESTAMP.log

# FunÃ§Ã£o para executar testes com retry
run_tests_with_retry() {
    local test_name="$1"
    local test_command="$2"
    local max_retries=3
    local retry_count=0

    while [ $retry_count -lt $max_retries ]; do
        print_status "Executando $test_name (tentativa $((retry_count + 1))/$max_retries)..."

        # Executar comando e capturar cÃ³digo de saÃ­da
        eval "$test_command"
        local exit_code=$?
        
        if [ $exit_code -eq 0 ]; then
            print_success "$test_name executado com sucesso"
            return 0
        else
            retry_count=$((retry_count + 1))
            if [ $retry_count -lt $max_retries ]; then
                print_warning "$test_name falhou (cÃ³digo $exit_code), tentando novamente..."
                sleep 2
            else
                print_error "$test_name falhou apÃ³s $max_retries tentativas (cÃ³digo $exit_code)"
                return 1
            fi
        fi
    done
}

# Contadores
TESTS_PASSED=0
TESTS_FAILED=0

# 1. Testes de Modelos (UnitÃ¡rios)
echo
echo "ðŸ§ª 1. TESTES DE MODELOS (UNITÃRIOS)"
echo "=================================="

if run_tests_with_retry "Testes de Modelos" "pytest tests/test_models.py -v --tb=short --junitxml=$REPORTS_DIR/models_$TIMESTAMP.xml"; then
    ((TESTS_PASSED++))
else
    ((TESTS_FAILED++))
fi

# 2. Testes de AutenticaÃ§Ã£o
echo
echo "ðŸ” 2. TESTES DE AUTENTICAÃ‡ÃƒO"
echo "============================"

if run_tests_with_retry "Testes de AutenticaÃ§Ã£o" "pytest tests/test_auth.py -v --tb=short --junitxml=$REPORTS_DIR/auth_$TIMESTAMP.xml"; then
    ((TESTS_PASSED++))
else
    ((TESTS_FAILED++))
fi

# 3. Testes de Rotas Administrativas
echo
echo "ðŸ‘‘ 3. TESTES DE ROTAS ADMINISTRATIVAS"
echo "===================================="

if run_tests_with_retry "Testes Admin Routes" "pytest tests/test_admin_routes.py -v --tb=short --junitxml=$REPORTS_DIR/admin_$TIMESTAMP.xml"; then
    ((TESTS_PASSED++))
else
    ((TESTS_FAILED++))
fi

# 4. Testes de IntegraÃ§Ã£o
echo
echo "ðŸ”— 4. TESTES DE INTEGRAÃ‡ÃƒO"
echo "=========================="

if run_tests_with_retry "Testes de IntegraÃ§Ã£o" "pytest tests/test_integration.py -v --tb=short --junitxml=$REPORTS_DIR/integration_$TIMESTAMP.xml"; then
    ((TESTS_PASSED++))
else
    ((TESTS_FAILED++))
fi

# 5. Testes com Cobertura Completa
echo
echo "ðŸ“Š 5. ANÃLISE DE COBERTURA"
echo "=========================="

print_status "Executando todos os testes com anÃ¡lise de cobertura..."

if pytest --cov=. --cov-report=html:htmlcov --cov-report=xml:$REPORTS_DIR/coverage_$TIMESTAMP.xml --cov-report=term-missing --cov-fail-under=70 --junitxml=$REPORTS_DIR/all_tests_$TIMESTAMP.xml; then
    print_success "AnÃ¡lise de cobertura concluÃ­da"
    print_status "RelatÃ³rio HTML disponÃ­vel em: htmlcov/index.html"
    ((TESTS_PASSED++))
else
    print_warning "AnÃ¡lise de cobertura falhou ou cobertura abaixo do mÃ­nimo"
    ((TESTS_FAILED++))
fi

# 6. VerificaÃ§Ãµes de Qualidade
echo
echo "ðŸ” 6. VERIFICAÃ‡Ã•ES DE QUALIDADE"
echo "==============================="

# Flake8
print_status "Executando Flake8..."
if flake8 . --count --statistics --max-line-length=127 --output-file=$REPORTS_DIR/flake8_$TIMESTAMP.txt; then
    print_success "Flake8 passou"
else
    print_warning "Flake8 encontrou problemas"
fi

# Black
print_status "Verificando formataÃ§Ã£o com Black..."
if black --check --diff . >$REPORTS_DIR/black_$TIMESTAMP.txt; then
    print_success "CÃ³digo bem formatado"
else
    print_warning "CÃ³digo precisa de formataÃ§Ã£o"
fi

# isort
print_status "Verificando imports com isort..."
if isort --check-only --diff . >$REPORTS_DIR/isort_$TIMESTAMP.txt; then
    print_success "Imports bem organizados"
else
    print_warning "Imports precisam de organizaÃ§Ã£o"
fi

# 7. VerificaÃ§Ãµes de SeguranÃ§a
echo
echo "ðŸ”’ 7. VERIFICAÃ‡Ã•ES DE SEGURANÃ‡A"
echo "==============================="

# Bandit
if command_exists bandit; then
    print_status "Executando Bandit (anÃ¡lise de seguranÃ§a)..."
    if bandit -r . -f json -o $REPORTS_DIR/bandit_$TIMESTAMP.json; then
        print_success "Bandit concluÃ­do"
    else
        print_warning "Bandit encontrou problemas potenciais"
    fi
else
    print_warning "Bandit nÃ£o instalado"
fi

# Safety
if command_exists safety; then
    print_status "Executando Safety (vulnerabilidades de dependÃªncias)..."
    if safety check --json --output $REPORTS_DIR/safety_$TIMESTAMP.json; then
        print_success "Safety concluÃ­do"
    else
        print_warning "Safety encontrou vulnerabilidades"
    fi
else
    print_warning "Safety nÃ£o instalado"
fi

# 8. Testes de Performance (se disponÃ­veis)
echo
echo "âš¡ 8. TESTES DE PERFORMANCE"
echo "=========================="

if pytest tests/test_integration.py::TestPerformance -v --benchmark-only --benchmark-json=$REPORTS_DIR/benchmark_$TIMESTAMP.json 2>/dev/null; then
    print_success "Testes de performance concluÃ­dos"
else
    print_warning "Testes de performance nÃ£o disponÃ­veis ou falharam"
fi

# 9. Gerar RelatÃ³rio Consolidado
echo
echo "ðŸ“„ 9. RELATÃ“RIO CONSOLIDADO"
echo "=========================="

REPORT_FILE="$REPORTS_DIR/consolidated_report_$TIMESTAMP.md"

cat >$REPORT_FILE <<EOF
# RelatÃ³rio de Testes - SIA2

**Data/Hora:** $(date)
**Timestamp:** $TIMESTAMP

## Resumo Executivo

- âœ… Testes Passaram: $TESTS_PASSED
- âŒ Testes Falharam: $TESTS_FAILED
- ðŸ“Š Taxa de Sucesso: $((TESTS_PASSED * 100 / (TESTS_PASSED + TESTS_FAILED)))%

## Detalhes dos Testes

### 1. Testes UnitÃ¡rios (Modelos)
- Arquivo: \`tests/test_models.py\`
- RelatÃ³rio: \`models_$TIMESTAMP.xml\`

### 2. Testes de AutenticaÃ§Ã£o
- Arquivo: \`tests/test_auth.py\`
- RelatÃ³rio: \`auth_$TIMESTAMP.xml\`

### 3. Testes de Rotas Administrativas
- Arquivo: \`tests/test_admin_routes.py\`
- RelatÃ³rio: \`admin_$TIMESTAMP.xml\`

### 4. Testes de IntegraÃ§Ã£o
- Arquivo: \`tests/test_integration.py\`
- RelatÃ³rio: \`integration_$TIMESTAMP.xml\`

### 5. Cobertura de CÃ³digo
- RelatÃ³rio HTML: \`htmlcov/index.html\`
- RelatÃ³rio XML: \`coverage_$TIMESTAMP.xml\`

## Qualidade de CÃ³digo

### Linting
- Flake8: \`flake8_$TIMESTAMP.txt\`
- Black: \`black_$TIMESTAMP.txt\`
- isort: \`isort_$TIMESTAMP.txt\`

### SeguranÃ§a
- Bandit: \`bandit_$TIMESTAMP.json\`
- Safety: \`safety_$TIMESTAMP.json\`

### Performance
- Benchmark: \`benchmark_$TIMESTAMP.json\`

## PrÃ³ximos Passos

1. Revisar testes que falharam
2. Verificar cobertura de cÃ³digo
3. Resolver problemas de qualidade identificados
4. Atualizar dependÃªncias com vulnerabilidades

EOF

print_success "RelatÃ³rio consolidado gerado: $REPORT_FILE"

# 10. Resumo Final
echo
echo "ðŸ“‹ RESUMO FINAL"
echo "==============="

echo "ðŸ“Š EstatÃ­sticas:"
echo "  â€¢ Testes executados: $((TESTS_PASSED + TESTS_FAILED))"
echo "  â€¢ Testes passaram: $TESTS_PASSED"
echo "  â€¢ Testes falharam: $TESTS_FAILED"

if [ $TESTS_FAILED -eq 0 ]; then
    print_success "ðŸŽ‰ Todos os testes passaram!"
    echo "âœ… Pipeline de testes concluÃ­do com sucesso"
    exit 0
else
    print_warning "âš ï¸  Alguns testes falharam"
    echo "ðŸ“ Verifique os relatÃ³rios em: $REPORTS_DIR"
    echo "ðŸ” RelatÃ³rio consolidado: $REPORT_FILE"
    exit 1
fi
